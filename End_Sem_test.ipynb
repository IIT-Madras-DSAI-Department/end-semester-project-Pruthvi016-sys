{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a19d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "766648bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, learning_rate=0.1, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _one_hot(self, y, num_classes):\n",
    "        return np.eye(num_classes)[y]\n",
    "\n",
    "    def _cross_entropy_loss(self, y_true, y_pred):\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_classes = np.max(y) + 1\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.W = np.random.randn(num_features, num_classes) * 0.01\n",
    "        self.b = np.zeros((1, num_classes))\n",
    "\n",
    "        # One-hot encode labels\n",
    "        Y_onehot = self._one_hot(y, num_classes)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Forward pass\n",
    "            logits = np.dot(X, self.W) + self.b\n",
    "            probs = self._softmax(logits)\n",
    "\n",
    "            # Loss (for monitoring)\n",
    "            loss = self._cross_entropy_loss(Y_onehot, probs)\n",
    "\n",
    "            # Backward pass - CORRECTED GRADIENTS\n",
    "            grad_logits = probs - Y_onehot\n",
    "            grad_W = np.dot(X.T, grad_logits) / num_samples\n",
    "            grad_b = np.sum(grad_logits, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "            # Update weights\n",
    "            self.W -= self.learning_rate * grad_W\n",
    "            self.b -= self.learning_rate * grad_b\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        logits = np.dot(X, self.W) + self.b\n",
    "        return self._softmax(logits)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77d02bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "train_data = pd.read_csv('MNIST_train.csv')\n",
    "val_data = pd.read_csv('MNIST_validation.csv')\n",
    "print(\"Loaded data successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c932852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10002, 786)\n",
      "(2499, 786)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5f2d9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "      <th>even</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10002 rows Ã— 786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x20  28x21  \\\n",
       "0          0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1          3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3          3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "9997       1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "9998       2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "9999       5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "10000      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "10001      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "       28x22  28x23  28x24  28x25  28x26  28x27  28x28  even  \n",
       "0          0      0      0      0      0      0      0     1  \n",
       "1          0      0      0      0      0      0      0     0  \n",
       "2          0      0      0      0      0      0      0     1  \n",
       "3          0      0      0      0      0      0      0     0  \n",
       "4          0      0      0      0      0      0      0     1  \n",
       "...      ...    ...    ...    ...    ...    ...    ...   ...  \n",
       "9997       0      0      0      0      0      0      0     0  \n",
       "9998       0      0      0      0      0      0      0     1  \n",
       "9999       0      0      0      0      0      0      0     0  \n",
       "10000      0      0      0      0      0      0      0     0  \n",
       "10001      0      0      0      0      0      0      0     0  \n",
       "\n",
       "[10002 rows x 786 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3871ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (10002, 784)\n",
      "Validation set: (2499, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data.iloc[:, 1:785].values / 255.0\n",
    "y_train = train_data.iloc[:, 0].values\n",
    "\n",
    "X_val = val_data.iloc[:, 1:785].values / 255.0\n",
    "y_val = val_data.iloc[:, 0].values\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c4dd219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SOFTMAX REGRESSION COMPLETE TESTING\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Starting training...\n",
      "Epoch 0: Loss = 2.3014\n",
      "Epoch 100: Loss = 0.6132\n",
      "Epoch 200: Loss = 0.4918\n",
      "Epoch 300: Loss = 0.4406\n",
      "Epoch 400: Loss = 0.4105\n",
      "\n",
      "âœ… Training completed in 9.27 seconds (0.15 minutes)\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test Softmax Regression\n",
    "print(\"=\"*60)\n",
    "print(\"SOFTMAX REGRESSION COMPLETE TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize model\n",
    "model = SoftmaxRegression(learning_rate=0.1, epochs=500)\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "\n",
    "# Validation\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "pred_time = time.time() - pred_start\n",
    "print(f\"âœ… Prediction completed in {pred_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f2f5e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMANCE METRICS\n",
      "============================================================\n",
      "F1 Score (Macro): 0.8892\n",
      "F1 Score (Weighted): 0.8902\n",
      "Accuracy: 0.8908\n",
      "Total Training Time: 9.27s (0.15min)\n",
      "Prediction Time: 0.01s\n",
      "\n",
      "â° TIME ANALYSIS FOR ENSEMBLE PLANNING:\n",
      "Softmax used: 9.27s\n",
      "Remaining budget: 290.73s (4.85min)\n",
      "âœ… Excellent! You have plenty of time for 2-3 additional models.\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics using sklearn\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# F1 Scores\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_val, y_pred, average='weighted')\n",
    "f1_per_class = f1_score(y_val, y_pred, average=None)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_val)\n",
    "\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Total Training Time: {training_time:.2f}s ({training_time/60:.2f}min)\")\n",
    "print(f\"Prediction Time: {pred_time:.2f}s\")\n",
    "\n",
    "# Time analysis for ensemble planning\n",
    "remaining_time = 300 - training_time  # 5 minutes = 300 seconds\n",
    "print(f\"\\nâ° TIME ANALYSIS FOR ENSEMBLE PLANNING:\")\n",
    "print(f\"Softmax used: {training_time:.2f}s\")\n",
    "print(f\"Remaining budget: {remaining_time:.2f}s ({remaining_time/60:.2f}min)\")\n",
    "\n",
    "if remaining_time > 60:\n",
    "    print(\"âœ… Excellent! You have plenty of time for 2-3 additional models.\")\n",
    "elif remaining_time > 0:\n",
    "    print(\"âš ï¸  Tight! You can add 1-2 fast models only.\")\n",
    "else:\n",
    "    print(\"âŒ Time exceeded! Need to reduce epochs or use data subset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69df9973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘€ Sample Predictions (first 20 validation samples):\n",
      "Sample  0: Actual=4, Predicted=4 âœ…\n",
      "Sample  1: Actual=6, Predicted=6 âœ…\n",
      "Sample  2: Actual=7, Predicted=7 âœ…\n",
      "Sample  3: Actual=2, Predicted=2 âœ…\n",
      "Sample  4: Actual=5, Predicted=5 âœ…\n",
      "Sample  5: Actual=6, Predicted=6 âœ…\n",
      "Sample  6: Actual=4, Predicted=4 âœ…\n",
      "Sample  7: Actual=8, Predicted=8 âœ…\n",
      "Sample  8: Actual=4, Predicted=4 âœ…\n",
      "Sample  9: Actual=5, Predicted=5 âœ…\n",
      "Sample 10: Actual=1, Predicted=1 âœ…\n",
      "Sample 11: Actual=1, Predicted=1 âœ…\n",
      "Sample 12: Actual=3, Predicted=3 âœ…\n",
      "Sample 13: Actual=3, Predicted=2 âŒ\n",
      "Sample 14: Actual=8, Predicted=8 âœ…\n",
      "Sample 15: Actual=6, Predicted=6 âœ…\n",
      "Sample 16: Actual=9, Predicted=9 âœ…\n",
      "Sample 17: Actual=4, Predicted=4 âœ…\n",
      "Sample 18: Actual=6, Predicted=6 âœ…\n",
      "Sample 19: Actual=5, Predicted=5 âœ…\n",
      "\n",
      "ðŸ“ˆ Error Rate: 0.1092 (273 errors out of 2499)\n"
     ]
    }
   ],
   "source": [
    "# Sample predictions visualization\n",
    "print(\"\\nðŸ‘€ Sample Predictions (first 20 validation samples):\")\n",
    "sample_indices = range(20)\n",
    "sample_actual = y_val[sample_indices]\n",
    "sample_pred = y_pred[sample_indices]\n",
    "\n",
    "for i, (actual, pred) in enumerate(zip(sample_actual, sample_pred)):\n",
    "    status = \"âœ…\" if actual == pred else \"âŒ\"\n",
    "    print(f\"Sample {i:2d}: Actual={actual}, Predicted={pred} {status}\")\n",
    "\n",
    "# Error analysis\n",
    "errors = y_pred != y_val\n",
    "error_rate = np.mean(errors)\n",
    "print(f\"\\nðŸ“ˆ Error Rate: {error_rate:.4f} ({np.sum(errors)} errors out of {len(y_val)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0235f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i, x in enumerate(X):\n",
    "            # Compute distances to all training points\n",
    "            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
    "            \n",
    "            # Get k nearest neighbors\n",
    "            nearest_indices = np.argsort(distances)[:self.k]\n",
    "            nearest_labels = self.y_train[nearest_indices]\n",
    "            \n",
    "            # Majority vote\n",
    "            most_common = Counter(nearest_labels).most_common(1)[0][0]\n",
    "            predictions.append(most_common)\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                print(f\"Processed {i}/{len(X)} samples...\")\n",
    "                \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02b6af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KNN MODEL TESTING - FULL DATASET\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Starting KNN 'training' (storing data)...\n",
      "âœ… 'Training' completed in 0.0000 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "Processed 0/2499 samples...\n",
      "Processed 500/2499 samples...\n",
      "Processed 1000/2499 samples...\n",
      "Processed 1500/2499 samples...\n",
      "Processed 2000/2499 samples...\n",
      "âœ… Prediction completed in 176.12 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test KNN on FULL dataset\n",
    "print(\"=\"*60)\n",
    "print(\"KNN MODEL TESTING - FULL DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize KNN\n",
    "knn = KNN(k=5)\n",
    "\n",
    "# Training (KNN just stores the data)\n",
    "print(\"\\nðŸš€ Starting KNN 'training' (storing data)...\")\n",
    "train_start = time.time()\n",
    "knn.fit(X_train, y_train)  # Use FULL training data\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… 'Training' completed in {training_time:.4f} seconds\")\n",
    "\n",
    "# Prediction (this is where KNN takes time)\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_knn = knn.predict(X_val)  # Predict on FULL validation set\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "76a2fe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OptimizedWeightedKNN...\n",
      "WeightedKNN Results:\n",
      "F1 Score: 0.9515 \n",
      "Training Time: 0.0187s\n",
      "Prediction Time: 1.2661s\n",
      "Total Time: 1.2847s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "class OptimizedWeightedKNN:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Convert to float32 for faster computation\n",
    "        self.X_train = X.astype(np.float32)\n",
    "        self.y_train = y.astype(np.int32)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Convert to float32 for faster computation\n",
    "        X = X.astype(np.float32)\n",
    "        predictions = np.empty(X.shape[0], dtype=np.int32)\n",
    "        \n",
    "        # Precompute squared norms for training data\n",
    "        train_norms = np.sum(self.X_train ** 2, axis=1)\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Use squared Euclidean distance\n",
    "            test_norm = np.sum(X[i] ** 2)\n",
    "            dot_product = np.dot(self.X_train, X[i])\n",
    "            squared_distances = train_norms + test_norm - 2 * dot_product\n",
    "            \n",
    "            # Get k nearest neighbors\n",
    "            nearest_indices = np.argpartition(squared_distances, self.k)[:self.k]\n",
    "            nearest_distances = squared_distances[nearest_indices]\n",
    "            nearest_labels = self.y_train[nearest_indices]\n",
    "            \n",
    "            # Convert squared distances to actual distances for weighting\n",
    "            distances = np.sqrt(nearest_distances + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "            \n",
    "            # Calculate weights (inverse distance weighting)\n",
    "            weights = 1.0 / (distances + 1e-8)\n",
    "            \n",
    "            # Weighted majority voting\n",
    "            weighted_votes = {}\n",
    "            for label, weight in zip(nearest_labels, weights):\n",
    "                if label in weighted_votes:\n",
    "                    weighted_votes[label] += weight\n",
    "                else:\n",
    "                    weighted_votes[label] = weight\n",
    "            \n",
    "            # Find label with highest total weight\n",
    "            predictions[i] = max(weighted_votes.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "class BatchOptimizedWeightedKNN:\n",
    "    def __init__(self, k=5, batch_size=100):\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X.astype(np.float32)\n",
    "        self.y_train = y.astype(np.int32)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = X.astype(np.float32)\n",
    "        predictions = np.empty(X.shape[0], dtype=np.int32)\n",
    "        \n",
    "        # Precompute squared norms for training data\n",
    "        train_norms = np.sum(self.X_train ** 2, axis=1)\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, X.shape[0], self.batch_size):\n",
    "            batch_end = min(batch_start + self.batch_size, X.shape[0])\n",
    "            X_batch = X[batch_start:batch_end]\n",
    "            \n",
    "            # Vectorized distance computation\n",
    "            test_norms = np.sum(X_batch ** 2, axis=1)\n",
    "            dot_products = np.dot(self.X_train, X_batch.T)\n",
    "            squared_distances = train_norms[:, np.newaxis] + test_norms - 2 * dot_products\n",
    "            \n",
    "            # Get k nearest neighbors for each batch sample\n",
    "            nearest_indices = np.argpartition(squared_distances, self.k, axis=0)[:self.k]\n",
    "            nearest_squared_distances = np.take_along_axis(squared_distances, nearest_indices, axis=0)\n",
    "            nearest_labels = self.y_train[nearest_indices]\n",
    "            \n",
    "            # Convert to actual distances and compute weights\n",
    "            distances = np.sqrt(nearest_squared_distances + 1e-8)\n",
    "            weights = 1.0 / (distances + 1e-8)\n",
    "            \n",
    "            # Weighted majority voting for each sample in batch\n",
    "            for j in range(batch_end - batch_start):\n",
    "                label_weights = {}\n",
    "                for idx in range(self.k):\n",
    "                    label = nearest_labels[idx, j]\n",
    "                    weight = weights[idx, j]\n",
    "                    label_weights[label] = label_weights.get(label, 0.0) + weight\n",
    "                \n",
    "                predictions[batch_start + j] = max(label_weights.items(), key=lambda x: x[1])[0]\n",
    "                \n",
    "        return predictions\n",
    "\n",
    "# Test the optimized weighted KNN\n",
    "print(\"Testing OptimizedWeightedKNN...\")\n",
    "weighted_knn = BatchOptimizedWeightedKNN(k=5, batch_size=200)\n",
    "\n",
    "start_time = time.time()\n",
    "weighted_knn.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "pred_start = time.time()\n",
    "y_pred_weighted = weighted_knn.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "f1_weighted = f1_score(y_val, y_pred_weighted, average='weighted')\n",
    "\n",
    "print(f\"WeightedKNN Results:\")\n",
    "print(f\"F1 Score: {f1_weighted:.4f} \")\n",
    "print(f\"Training Time: {training_time:.4f}s\")\n",
    "print(f\"Prediction Time: {pred_time:.4f}s\")\n",
    "print(f\"Total Time: {training_time + pred_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3c911d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i, x in enumerate(X):\n",
    "            # Compute distances to all training points\n",
    "            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
    "            \n",
    "            # Get k nearest neighbors\n",
    "            nearest_indices = np.argsort(distances)[:self.k]\n",
    "            nearest_labels = self.y_train[nearest_indices]\n",
    "            \n",
    "            # Majority vote\n",
    "            most_common = Counter(nearest_labels).most_common(1)[0][0]\n",
    "            predictions.append(most_common)\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                print(f\"Processed {i}/{len(X)} samples...\")\n",
    "                \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6fb3b211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "KNN PERFORMANCE METRICS\n",
      "============================================================\n",
      "F1 Score (Weighted): 0.9523\n",
      "Training Time: 0.0187s\n",
      "Prediction Time: 1.27s\n",
      "\n",
      "ðŸ†š COMPARISON WITH SOFTMAX:\n",
      "Softmax F1 (Weighted): 0.8902\n",
      "KNN F1 (Weighted):     0.9523\n",
      "Softmax Training: 7.92s\n",
      "KNN Training: 0.0187s\n",
      "KNN Prediction: 1.27s\n",
      "\n",
      "â° REMAINING TIME BUDGET: 290.80s (4.85min)\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KNN PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "f1_weighted_knn = f1_score(y_val, y_pred_knn, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_knn:.4f}\")\n",
    "print(f\"Training Time: {training_time:.4f}s\")\n",
    "print(f\"Prediction Time: {pred_time:.2f}s\")\n",
    "\n",
    "# Compare with Softmax\n",
    "print(f\"\\nðŸ†š COMPARISON WITH SOFTMAX:\")\n",
    "print(f\"Softmax F1 (Weighted): 0.8902\")\n",
    "print(f\"KNN F1 (Weighted):     {f1_weighted_knn:.4f}\")\n",
    "print(f\"Softmax Training: 7.92s\")\n",
    "print(f\"KNN Training: {training_time:.4f}s\")\n",
    "print(f\"KNN Prediction: {pred_time:.2f}s\")\n",
    "\n",
    "# Time analysis\n",
    "total_knn_time = training_time + pred_time\n",
    "remaining_time = 300 - 7.92 - total_knn_time  # Subtract Softmax + KNN times\n",
    "print(f\"\\nâ° REMAINING TIME BUDGET: {remaining_time:.2f}s ({remaining_time/60:.2f}min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aaee0b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common errors: 85\n",
      "Different errors: 222\n"
     ]
    }
   ],
   "source": [
    "# Quick check - do KNN and Softmax make different errors?\n",
    "common_errors = np.sum((y_pred != y_val) & (y_pred_knn != y_val))\n",
    "different_errors = np.sum((y_pred != y_val) ^ (y_pred_knn != y_val))\n",
    "print(f\"Common errors: {common_errors}\")\n",
    "print(f\"Different errors: {different_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "217c6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Decision Tree Node -----------\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index  # index of feature to split on\n",
    "        self.threshold = threshold          # threshold value to split\n",
    "        self.left = left                    # left subtree\n",
    "        self.right = right                  # right subtree\n",
    "        self.value = value                  # class label for leaf nodes\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        # returns true if this node hold a value\n",
    "        return self.value is not None\n",
    "# ----------- Decision Tree Classifier -----------s\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2, feature_indices = None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # This is an extra line from earlier\n",
    "        # the indices of features to be used are passed as argument\n",
    "        self.feature_indices = feature_indices\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #print ('in fit function, type of X is', type(X))\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # This is an different from earlier\n",
    "        # if subset of features is not defined\n",
    "        if not self.feature_indices:\n",
    "            self.feature_indices = list(np.arange(len(X[0])))\n",
    "            assert len(self.feature_indices) > 1, 'less than 2 features, tree building may fail'\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    # additional argument \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_classes = len(set(y))\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # base case 1\n",
    "        # This is an different from earlier\n",
    "        num_samples = len(y)\n",
    "        if (num_samples == 0):\n",
    "            return None\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # base case 2\n",
    "        # stopping conditions\n",
    "        if (depth >= self.max_depth or\n",
    "            num_classes == 1 or\n",
    "            num_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "            \n",
    "        # greedy search for best split\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # feat_idxs instead of num_features in this line\n",
    "        best_feat, best_thresh = self._best_split(X, y)\n",
    "        #print ('best feat and best thresh are', best_feat, best_thresh)\n",
    "        # base case 3 - best split not found \n",
    "        if best_feat is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            #print ('best feat is none, returning common label', leaf_value)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "\n",
    "        # recursive case \n",
    "        # split is found\n",
    "        left_idx = X[:, best_feat] <= best_thresh\n",
    "        right_idx = X[:, best_feat] > best_thresh\n",
    "        #print ('left_idx and right idx are',sum(left_idx), sum(right_idx))\n",
    "\n",
    "        # if one of the parts is empty\n",
    "        if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:\n",
    "            # If either side is empty, return a leaf node with the most common label\n",
    "            leaf_value = self.most_common_label(y)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "            \n",
    "        left = self._build_tree(X[left_idx], y[left_idx], depth + 1)\n",
    "        right = self._build_tree(X[right_idx], y[right_idx], depth + 1)\n",
    "        return DecisionTreeNode(feature_index=best_feat, threshold=best_thresh, left=left, right=right)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = 0\n",
    "        split_idx, split_thresh = None, None\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # feat_idxs instead of num_features in this line\n",
    "        for feat_idx in self.feature_indices:\n",
    "            #print ('feat idx is', feat_idx)\n",
    "            #print ('type of X is ', type(X))\n",
    "            thresholds = np.unique(X[:, feat_idx])\n",
    "            #print ('thresholds', thresholds)\n",
    "            for thresh in thresholds:\n",
    "                #print ('each thresh is', thresh)\n",
    "                gain = self._gini_gain(y, X[:, feat_idx], thresh)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_thresh = thresh\n",
    "            #print ('completed for loop')\n",
    "        #print ('best split is', split_idx, split_thresh, best_gain)\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _gini_gain(self, y, feature_column, threshold):\n",
    "        # parent gini\n",
    "        parent_gini = self._gini(y)\n",
    "\n",
    "        # generate splits\n",
    "        left_idx = feature_column <= threshold\n",
    "        right_idx = feature_column > threshold\n",
    "\n",
    "        if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
    "            return 0\n",
    "\n",
    "        # weighted avg. gini of children\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y[left_idx]), len(y[right_idx])\n",
    "        gini_left = self._gini(y[left_idx])\n",
    "        gini_right = self._gini(y[right_idx])\n",
    "        child_gini = (n_left / n) * gini_left + (n_right / n) * gini_right\n",
    "\n",
    "        # gini gain\n",
    "        return parent_gini - child_gini\n",
    "\n",
    "    def _gini(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1.0 - sum(p**2 for p in probabilities if p > 0)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        if set(y) is None:\n",
    "            return None\n",
    "        #print ('unique y values are', set(y))\n",
    "        counter = Counter(y)\n",
    "        #print ('in most common label', counter)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        #print ('in most common label', counter, most_common)\n",
    "        return most_common\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs, self.root) for inputs in X])\n",
    "\n",
    "    def _predict(self, inputs, node):\n",
    "        # base case\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        # recursive calling of left and right branches\n",
    "        if inputs[node.feature_index] <= node.threshold:\n",
    "            return self._predict(inputs, node.left)\n",
    "        else:\n",
    "            return self._predict(inputs, node.right)\n",
    "# ----------- Random Forest Classifier -----------\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, max_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        self.max_features = self.max_features or len(X[0])\n",
    "        \n",
    "        for tci in range(self.n_trees):\n",
    "            print ('~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "            print ('builiding tree', tci)\n",
    "            \n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            \n",
    "            # selection feature indices\n",
    "            feature_indices = random.sample(range(len(X[0])), self.max_features)\n",
    "            print ('in tree, feature indices are', tci, feature_indices)\n",
    "            \n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                feature_indices = feature_indices\n",
    "            )\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "            print ('completed building tree', tci)\n",
    "            print ('~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        #print ('in bootstrap, type of X is', type(X))\n",
    "        n_samples = len(X)\n",
    "        indices = [random.randint(0, n_samples - 1) for _ in range(n_samples)]\n",
    "        #print ('length of samples and indices is', n_samples, len(indices))\n",
    "        return np.array([X[i] for i in indices]), np.array([y[i] for i in indices])\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = [tree.predict(X) for tree in self.trees]\n",
    "        print ('before transformation, tree_preds, length and type', len(tree_preds), type(tree_preds), np.shape(tree_preds))\n",
    "        tree_preds = list(zip(*tree_preds))\n",
    "        print ('after transformation, tree_preds, length and type', len(tree_preds), type(tree_preds), np.shape(tree_preds))\n",
    "        return [self._most_common_label(preds) for preds in tree_preds]\n",
    "\n",
    "    def _most_common_label(self, labels):\n",
    "        return Counter(labels).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5e5110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM FOREST MODEL TESTING - FULL DATASET\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Starting Random Forest training...\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "builiding tree 0\n",
      "in tree, feature indices are 0 [754, 301, 611, 235, 328, 647, 381, 547, 443, 659, 291, 697, 32, 615, 486, 251, 165, 102, 579, 682, 341, 254, 442, 481, 266, 427, 391, 610, 404, 221, 274, 304, 332, 42, 738, 529, 259, 664, 249, 613, 530, 409, 347, 238, 513, 280, 326, 179, 540, 531, 22, 535, 35, 227, 142, 211, 466, 93, 623, 205, 480, 686, 639, 177, 464, 641, 576, 489, 702, 581, 164, 345, 424, 101, 209, 353, 773, 721, 484, 572, 690, 299, 420, 377, 116, 521, 210, 445, 365, 394, 358, 661, 124, 414, 150, 747, 618, 778, 582, 441]\n",
      "completed building tree 0\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "builiding tree 1\n",
      "in tree, feature indices are 1 [239, 523, 547, 658, 260, 778, 278, 454, 327, 606, 369, 465, 17, 690, 370, 618, 684, 301, 521, 226, 609, 227, 575, 435, 631, 276, 308, 137, 641, 174, 122, 358, 8, 434, 645, 249, 0, 6, 636, 402, 204, 79, 568, 508, 607, 351, 682, 736, 222, 326, 624, 324, 410, 16, 106, 362, 73, 25, 515, 457, 440, 275, 450, 242, 522, 26, 760, 9, 43, 161, 284, 306, 558, 48, 669, 101, 113, 350, 296, 442, 132, 644, 368, 652, 776, 353, 150, 357, 742, 573, 30, 574, 779, 44, 406, 76, 72, 104, 495, 146]\n",
      "completed building tree 1\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "builiding tree 2\n",
      "in tree, feature indices are 2 [442, 671, 153, 358, 294, 380, 128, 402, 424, 271, 45, 124, 586, 383, 4, 496, 500, 452, 325, 548, 554, 718, 602, 43, 675, 89, 518, 659, 579, 700, 780, 666, 155, 767, 346, 469, 651, 400, 58, 750, 110, 77, 619, 580, 219, 59, 509, 555, 538, 203, 504, 406, 575, 68, 8, 474, 83, 169, 121, 178, 99, 363, 280, 337, 753, 576, 319, 229, 760, 323, 162, 605, 355, 403, 245, 237, 378, 207, 734, 716, 33, 257, 526, 28, 381, 636, 669, 92, 645, 630, 756, 549, 553, 561, 431, 35, 262, 587, 130, 527]\n",
      "completed building tree 2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "builiding tree 3\n",
      "in tree, feature indices are 3 [556, 412, 546, 429, 612, 554, 155, 51, 33, 199, 231, 722, 239, 309, 701, 397, 289, 181, 609, 122, 337, 177, 515, 500, 773, 696, 76, 28, 357, 143, 186, 421, 380, 464, 7, 583, 431, 638, 779, 594, 197, 161, 13, 370, 508, 768, 727, 396, 633, 324, 361, 62, 160, 89, 252, 548, 771, 661, 79, 669, 580, 758, 6, 440, 304, 742, 715, 461, 323, 693, 495, 781, 654, 74, 617, 163, 369, 394, 408, 169, 623, 709, 734, 359, 195, 636, 670, 53, 684, 533, 257, 157, 593, 342, 557, 745, 512, 406, 69, 57]\n",
      "completed building tree 3\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "builiding tree 4\n",
      "in tree, feature indices are 4 [663, 298, 75, 609, 328, 548, 389, 737, 158, 331, 28, 725, 131, 595, 673, 693, 606, 312, 168, 462, 577, 500, 556, 672, 482, 280, 217, 176, 622, 248, 116, 579, 324, 447, 423, 240, 721, 182, 93, 225, 154, 723, 431, 433, 514, 489, 229, 549, 292, 48, 542, 680, 295, 458, 167, 457, 755, 382, 739, 648, 413, 551, 750, 479, 180, 429, 552, 715, 266, 264, 507, 554, 561, 282, 512, 24, 778, 407, 547, 527, 135, 391, 564, 359, 259, 316, 251, 67, 452, 774, 704, 124, 57, 428, 730, 759, 257, 37, 50, 707]\n",
      "completed building tree 4\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "âœ… Training completed in 156.67 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "before transformation, tree_preds, length and type 5 <class 'list'> (5, 2499)\n",
      "after transformation, tree_preds, length and type 2499 <class 'list'> (2499, 5)\n",
      "âœ… Prediction completed in 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Test Random Forest on FULL dataset\n",
    "print(\"=\"*60)\n",
    "print(\"RANDOM FOREST MODEL TESTING - FULL DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize Random Forest with smaller settings for speed\n",
    "rf = RandomForest(\n",
    "    n_trees=5,           # Smaller number of trees for speed\n",
    "    max_depth=10,        # Limit depth\n",
    "    min_samples_split=20, # Larger min samples for speed\n",
    "    max_features=100     # Use only 100 features per tree for speed\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting Random Forest training...\")\n",
    "train_start = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "34024329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RANDOM FOREST PERFORMANCE METRICS\n",
      "============================================================\n",
      "F1 Score (Weighted): 0.8623\n",
      "Training Time: 156.67s\n",
      "Prediction Time: 0.04s\n",
      "\n",
      "ðŸ†š COMPARISON WITH ALL MODELS:\n",
      "Softmax F1: 0.8899\n",
      "KNN F1:     0.9523\n",
      "Random Forest F1: 0.8623\n",
      "\n",
      "â° REMAINING TIME BUDGET: -28.19s (-0.47min)\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "f1_weighted_rf = f1_score(y_val, y_pred_rf, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_rf:.4f}\")\n",
    "print(f\"Training Time: {training_time:.2f}s\")\n",
    "print(f\"Prediction Time: {pred_time:.2f}s\")\n",
    "\n",
    "# Compare with previous models\n",
    "print(f\"\\nðŸ†š COMPARISON WITH ALL MODELS:\")\n",
    "print(f\"Softmax F1: 0.8899\")\n",
    "print(f\"KNN F1:     0.9523\") \n",
    "print(f\"Random Forest F1: {f1_weighted_rf:.4f}\")\n",
    "\n",
    "# Time analysis\n",
    "total_rf_time = training_time + pred_time\n",
    "total_used = 8.11 + 163.38 + total_rf_time\n",
    "remaining_time = 300 - total_used\n",
    "print(f\"\\nâ° REMAINING TIME BUDGET: {remaining_time:.2f}s ({remaining_time/60:.2f}min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "523d6842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NAIVE BAYES MODEL TESTING\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Starting Naive Bayes training...\n",
      "âœ… Training completed in 0.1335 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.2388 seconds\n"
     ]
    }
   ],
   "source": [
    "class GaussianNB:\n",
    "    def __init__(self):\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "        self.priors = None\n",
    "        self.classes = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.variances = np.zeros((n_classes, n_features))\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        \n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.means[i] = X_c.mean(axis=0)\n",
    "            self.variances[i] = X_c.var(axis=0) + 1e-9  # Smoothing to avoid division by zero\n",
    "            self.priors[i] = X_c.shape[0] / n_samples\n",
    "            \n",
    "    def predict(self, X):\n",
    "        posteriors = []\n",
    "        for i in range(len(self.classes)):\n",
    "            prior = np.log(self.priors[i])\n",
    "            # Gaussian Naive Bayes: log(P(x|y)) = sum(log( GaussianPDF(x_i|mean_i, var_i) ))\n",
    "            # Simplified: -0.5 * sum( (x - mean)^2 / variance + log(2*pi*variance) )\n",
    "            nll = -0.5 * np.sum(np.log(2. * np.pi * self.variances[i]))\n",
    "            nll -= 0.5 * np.sum(((X - self.means[i]) ** 2) / self.variances[i], axis=1)\n",
    "            posterior = prior + nll\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        posteriors = np.array(posteriors).T\n",
    "        return self.classes[np.argmax(posteriors, axis=1)]\n",
    "\n",
    "# Test Naive Bayes\n",
    "print(\"=\"*60)\n",
    "print(\"NAIVE BAYES MODEL TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize Naive Bayes\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting Naive Bayes training...\")\n",
    "train_start = time.time()\n",
    "nb.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.4f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_nb = nb.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c5e6d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NAIVE BAYES PERFORMANCE METRICS\n",
      "============================================================\n",
      "F1 Score (Macro): 0.5351\n",
      "F1 Score (Weighted): 0.5424\n",
      "Accuracy: 0.5714\n",
      "Training Time: 0.1335s\n",
      "Prediction Time: 0.2388s\n",
      "\n",
      "ðŸ†š COMPARISON WITH ALL MODELS:\n",
      "Softmax F1: 0.8899\n",
      "KNN F1:     0.9523\n",
      "Random Forest F1: 0.8503\n",
      "Naive Bayes F1:   0.5424\n",
      "\n",
      "â° REMAINING TIME BUDGET: -6.80s (-0.11min)\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NAIVE BAYES PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "f1_macro_nb = f1_score(y_val, y_pred_nb, average='macro')\n",
    "f1_weighted_nb = f1_score(y_val, y_pred_nb, average='weighted')\n",
    "accuracy_nb = np.mean(y_pred_nb == y_val)\n",
    "\n",
    "print(f\"F1 Score (Macro): {f1_macro_nb:.4f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_nb:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_nb:.4f}\")\n",
    "print(f\"Training Time: {training_time:.4f}s\")\n",
    "print(f\"Prediction Time: {pred_time:.4f}s\")\n",
    "\n",
    "# Compare with all models\n",
    "print(f\"\\nðŸ†š COMPARISON WITH ALL MODELS:\")\n",
    "print(f\"Softmax F1: 0.8899\")\n",
    "print(f\"KNN F1:     0.9523\")\n",
    "print(f\"Random Forest F1: 0.8503\")\n",
    "print(f\"Naive Bayes F1:   {f1_weighted_nb:.4f}\")\n",
    "\n",
    "# Time analysis\n",
    "total_nb_time = training_time + pred_time\n",
    "total_used = 8.11 + 163.38 + 134.94 + total_nb_time\n",
    "remaining_time = 300 - total_used\n",
    "print(f\"\\nâ° REMAINING TIME BUDGET: {remaining_time:.2f}s ({remaining_time/60:.2f}min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd08a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SVM MODEL TESTING - ONE vs REST\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Starting SVM training (this may take a while)...\n",
      "Training SVM for class 0...\n",
      "Training SVM for class 1...\n",
      "Training SVM for class 2...\n",
      "Training SVM for class 3...\n",
      "Training SVM for class 4...\n",
      "Training SVM for class 5...\n",
      "Training SVM for class 6...\n",
      "Training SVM for class 7...\n",
      "Training SVM for class 8...\n",
      "Training SVM for class 9...\n",
      "âœ… Training completed in 202.50 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.0068 seconds\n"
     ]
    }
   ],
   "source": [
    "class MultiClassSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000, n_classes=10):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.n_classes = n_classes\n",
    "        self.W = None  # Weight matrix for all classes\n",
    "        self.b = None  # Bias vector for all classes\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.W = np.zeros((self.n_classes, n_features))\n",
    "        self.b = np.zeros(self.n_classes)\n",
    "\n",
    "        # One-vs-Rest approach\n",
    "        for class_idx in range(self.n_classes):\n",
    "            print(f\"Training SVM for class {class_idx}...\")\n",
    "            # Convert to binary labels: current class vs all others\n",
    "            y_binary = np.where(y == class_idx, 1, -1)\n",
    "            \n",
    "            w = np.zeros(n_features)\n",
    "            b = 0\n",
    "\n",
    "            for _ in range(self.n_iters):\n",
    "                for i, x_i in enumerate(X):\n",
    "                    condition = y_binary[i] * (np.dot(x_i, w) - b) >= 1\n",
    "                    if condition:\n",
    "                        w -= self.lr * (self.lambda_param * w)\n",
    "                    else:\n",
    "                        w -= self.lr * (self.lambda_param * w - np.dot(x_i, y_binary[i]))\n",
    "                        b -= self.lr * y_binary[i]\n",
    "\n",
    "            self.W[class_idx] = w\n",
    "            self.b[class_idx] = b\n",
    "\n",
    "    def predict(self, X):\n",
    "        # For each sample, find the class with highest score\n",
    "        scores = np.dot(X, self.W.T) - self.b\n",
    "        return np.argmax(scores, axis=1)\n",
    "\n",
    "# Test SVM\n",
    "print(\"=\"*60)\n",
    "print(\"SVM MODEL TESTING - ONE vs REST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize SVM with fewer iterations for speed\n",
    "svm = MultiClassSVM(\n",
    "    learning_rate=0.001,\n",
    "    lambda_param=0.01, \n",
    "    n_iters=100,  # Reduced for speed\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting SVM training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c8c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SVM PERFORMANCE METRICS\n",
      "============================================================\n",
      "F1 Score (Macro): 0.8922\n",
      "F1 Score (Weighted): 0.8934\n",
      "Accuracy: 0.8948\n",
      "Training Time: 202.50s\n",
      "Prediction Time: 0.0068s\n",
      "\n",
      "ðŸ†š COMPARISON WITH ALL MODELS:\n",
      "KNN F1:          0.9523\n",
      "Softmax F1:      0.8899\n",
      "Random Forest F1: 0.8503\n",
      "Naive Bayes F1:   0.5424\n",
      "SVM F1:          0.8934\n",
      "\n",
      "â° REMAINING TIME BUDGET: -208.94s (-3.48min)\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVM PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "f1_macro_svm = f1_score(y_val, y_pred_svm, average='macro')\n",
    "f1_weighted_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "accuracy_svm = np.mean(y_pred_svm == y_val)\n",
    "\n",
    "print(f\"F1 Score (Macro): {f1_macro_svm:.4f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_svm:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_svm:.4f}\")\n",
    "print(f\"Training Time: {training_time:.2f}s\")\n",
    "print(f\"Prediction Time: {pred_time:.4f}s\")\n",
    "\n",
    "# Compare with all models\n",
    "print(f\"\\nðŸ†š COMPARISON WITH ALL MODELS:\")\n",
    "print(f\"KNN F1:          0.9523\")\n",
    "print(f\"Softmax F1:      0.8899\")\n",
    "print(f\"Random Forest F1: 0.8503\")\n",
    "print(f\"Naive Bayes F1:   0.5424\")\n",
    "print(f\"SVM F1:          {f1_weighted_svm:.4f}\")\n",
    "\n",
    "# Time analysis\n",
    "total_svm_time = training_time + pred_time\n",
    "total_used = 8.11 + 163.38 + 134.94 + total_svm_time\n",
    "remaining_time = 300 - total_used\n",
    "print(f\"\\nâ° REMAINING TIME BUDGET: {remaining_time:.2f}s ({remaining_time/60:.2f}min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "42123911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "XGBOOST MODEL TESTING WITH SUBSAMPLING\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Starting XGBoost training (this may take a while)...\n",
      "Training XGBoost for class 0...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "Training XGBoost for class 1...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "Training XGBoost for class 2...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "Training XGBoost for class 3...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "Training XGBoost for class 4...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "Training XGBoost for class 5...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "Training XGBoost for class 6...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "Training XGBoost for class 7...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "Training XGBoost for class 8...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "Training XGBoost for class 9...\n",
      "Building tree 1/20...\n",
      "Building tree 2/20...\n",
      "Building tree 3/20...\n",
      "Building tree 4/20...\n",
      "Building tree 5/20...\n",
      "Building tree 6/20...\n",
      "Building tree 7/20...\n",
      "Building tree 8/20...\n",
      "Building tree 9/20...\n",
      "Building tree 10/20...\n",
      "Building tree 11/20...\n",
      "Building tree 12/20...\n",
      "Building tree 13/20...\n",
      "Building tree 14/20...\n",
      "Building tree 15/20...\n",
      "Building tree 16/20...\n",
      "Building tree 17/20...\n",
      "Building tree 18/20...\n",
      "Building tree 19/20...\n",
      "Building tree 20/20...\n",
      "âœ… Training completed in 387.86 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 8.8755 seconds\n",
      "\n",
      "ðŸ“Š XGBoost Performance:\n",
      "F1 Score (Weighted): 0.8878\n",
      "Accuracy: 0.8884\n",
      "Total Time: 396.73s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class XGBoostTree:\n",
    "    def __init__(self, max_depth=3, reg_lambda=1.0, gamma=0.0, max_bins=50):\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.max_bins = max_bins\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.leaf_value = None\n",
    "\n",
    "    def _calc_leaf_value(self, G, H):\n",
    "        return -G / (H + self.reg_lambda)\n",
    "\n",
    "    def _calc_gain(self, G_left, H_left, G_right, H_right, G_total, H_total):\n",
    "        def _term(G, H):\n",
    "            return G * G / (H + self.reg_lambda)\n",
    "        return 0.5 * (_term(G_left, H_left) + _term(G_right, H_right) - _term(G_total, H_total)) - self.gamma\n",
    "\n",
    "    def _find_best_split(self, X, g, h, sample_indices, feature_thresholds):\n",
    "        G_total = np.sum(g[sample_indices])\n",
    "        H_total = np.sum(h[sample_indices])\n",
    "        \n",
    "        best_gain = 0.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feat, thresholds in enumerate(feature_thresholds):\n",
    "            if thresholds is None:\n",
    "                continue\n",
    "\n",
    "            x_col = X[sample_indices, feat]\n",
    "            g_vals = g[sample_indices]\n",
    "            h_vals = h[sample_indices]\n",
    "\n",
    "            # Sort samples by feature value for efficient cumulative sums\n",
    "            sorted_idx = np.argsort(x_col)\n",
    "            x_sorted = x_col[sorted_idx]\n",
    "            g_sorted = g_vals[sorted_idx]\n",
    "            h_sorted = h_vals[sorted_idx]\n",
    "\n",
    "            # Precompute cumulative sums\n",
    "            cumsum_g = np.cumsum(g_sorted)\n",
    "            cumsum_h = np.cumsum(h_sorted)\n",
    "\n",
    "            # Try each threshold\n",
    "            for th in thresholds:\n",
    "                # Find split point: first index where x > th\n",
    "                split_pos = np.searchsorted(x_sorted, th, side='right')\n",
    "                if split_pos == 0 or split_pos == len(x_sorted):\n",
    "                    continue\n",
    "\n",
    "                G_left = cumsum_g[split_pos - 1]\n",
    "                H_left = cumsum_h[split_pos - 1]\n",
    "                G_right = G_total - G_left\n",
    "                H_right = H_total - H_left\n",
    "\n",
    "                gain = self._calc_gain(G_left, H_left, G_right, H_right, G_total, H_total)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feat\n",
    "                    best_threshold = th\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def _fit_recursive(self, X, g, h, sample_indices, depth, feature_thresholds):\n",
    "        n_samples = len(sample_indices)\n",
    "        G = np.sum(g[sample_indices])\n",
    "        H = np.sum(h[sample_indices])\n",
    "        self.leaf_value = self._calc_leaf_value(G, H)\n",
    "\n",
    "        if depth >= self.max_depth or n_samples < 2:\n",
    "            return\n",
    "\n",
    "        feature, threshold, gain = self._find_best_split(X, g, h, sample_indices, feature_thresholds)\n",
    "\n",
    "        if feature is None or gain <= 0:\n",
    "            return\n",
    "\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "\n",
    "        left_mask = X[sample_indices, feature] <= threshold\n",
    "        left_indices = sample_indices[left_mask]\n",
    "        right_indices = sample_indices[~left_mask]\n",
    "\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "            return\n",
    "\n",
    "        self.left = XGBoostTree(\n",
    "            max_depth=self.max_depth,\n",
    "            reg_lambda=self.reg_lambda,\n",
    "            gamma=self.gamma,\n",
    "            max_bins=self.max_bins\n",
    "        )\n",
    "        self.left._fit_recursive(X, g, h, left_indices, depth + 1, feature_thresholds)\n",
    "\n",
    "        self.right = XGBoostTree(\n",
    "            max_depth=self.max_depth,\n",
    "            reg_lambda=self.reg_lambda,\n",
    "            gamma=self.gamma,\n",
    "            max_bins=self.max_bins\n",
    "        )\n",
    "        self.right._fit_recursive(X, g, h, right_indices, depth + 1, feature_thresholds)\n",
    "\n",
    "    def fit(self, X, g, h, feature_thresholds):\n",
    "        sample_indices = np.arange(X.shape[0])\n",
    "        self._fit_recursive(X, g, h, sample_indices, 0, feature_thresholds)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.feature is None:\n",
    "            return np.full(X.shape[0], self.leaf_value)\n",
    "        left_mask = X[:, self.feature] <= self.threshold\n",
    "        right_mask = ~left_mask\n",
    "        preds = np.empty(X.shape[0])\n",
    "        preds[left_mask] = self.left.predict(X[left_mask]) if self.left else self.leaf_value\n",
    "        preds[right_mask] = self.right.predict(X[right_mask]) if self.right else self.leaf_value\n",
    "        return preds\n",
    "\n",
    "\n",
    "class XGBoostClassifier:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3, reg_lambda=1.0, gamma=0.0, max_bins=50, subsample=0.8):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.max_bins = max_bins\n",
    "        self.subsample = subsample\n",
    "        self.trees = []\n",
    "        self.base_score = 0.0\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        x = np.clip(x, -250, 250)\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _precompute_feature_thresholds(self, X):\n",
    "        thresholds = []\n",
    "        for feat in range(X.shape[1]):\n",
    "            col = X[:, feat]\n",
    "            if np.all(col == col[0]):\n",
    "                thresholds.append(None)\n",
    "                continue\n",
    "            unique_vals = np.unique(col)\n",
    "            if len(unique_vals) > self.max_bins:\n",
    "                percentiles = np.linspace(0, 100, self.max_bins + 1)\n",
    "                ths = np.percentile(col, percentiles[1:-1])\n",
    "                ths = np.unique(ths)\n",
    "            else:\n",
    "                ths = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n",
    "            thresholds.append(ths if len(ths) > 0 else None)\n",
    "        return thresholds\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        y_binary = y\n",
    "\n",
    "        # Precompute global thresholds once per dataset\n",
    "        feature_thresholds = self._precompute_feature_thresholds(X)\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        y_pred = np.full(n_samples, self.base_score, dtype=np.float64)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            print(f\"Building tree {i+1}/{self.n_estimators}...\")\n",
    "            \n",
    "            # SUBSAMPLING: Randomly select subset of data for this tree\n",
    "            if self.subsample < 1.0:\n",
    "                subsample_size = int(self.subsample * n_samples)\n",
    "                indices = np.random.choice(n_samples, subsample_size, replace=False)\n",
    "                X_sub = X[indices]\n",
    "                y_sub = y_binary[indices]\n",
    "                y_pred_sub = y_pred[indices]\n",
    "            else:\n",
    "                indices = np.arange(n_samples)\n",
    "                X_sub = X\n",
    "                y_sub = y_binary\n",
    "                y_pred_sub = y_pred\n",
    "\n",
    "            # Calculate gradients and hessians on subsampled data\n",
    "            p = self._sigmoid(y_pred_sub)\n",
    "            g = p - y_sub\n",
    "            h = p * (1.0 - p) + 1e-10\n",
    "\n",
    "            # Train tree on subsampled data\n",
    "            tree = XGBoostTree(\n",
    "                max_depth=self.max_depth,\n",
    "                reg_lambda=self.reg_lambda,\n",
    "                gamma=self.gamma,\n",
    "                max_bins=self.max_bins\n",
    "            )\n",
    "            tree.fit(X_sub, g, h, feature_thresholds)\n",
    "\n",
    "            # Update predictions on FULL dataset\n",
    "            update = tree.predict(X)\n",
    "            y_pred += self.learning_rate * update\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y_pred = np.full(X.shape[0], self.base_score, dtype=np.float64)\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        proba = self._sigmoid(y_pred)\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "class MultiClassXGBoost:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3, reg_lambda=1.0, gamma=0.0, max_bins=50, n_classes=10, subsample=0.8):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.max_bins = max_bins\n",
    "        self.n_classes = n_classes\n",
    "        self.subsample = subsample\n",
    "        self.classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classifiers = []\n",
    "        \n",
    "        # One-vs-Rest approach for multi-class\n",
    "        for class_idx in range(self.n_classes):\n",
    "            print(f\"Training XGBoost for class {class_idx}...\")\n",
    "            \n",
    "            # Convert to binary labels: current class vs all others\n",
    "            y_binary = (y == class_idx).astype(int)\n",
    "            \n",
    "            # Create and train binary classifier with subsampling\n",
    "            xgb_binary = XGBoostClassifier(\n",
    "                n_estimators=self.n_estimators,\n",
    "                learning_rate=self.learning_rate,\n",
    "                max_depth=self.max_depth,\n",
    "                reg_lambda=self.reg_lambda,\n",
    "                gamma=self.gamma,\n",
    "                max_bins=self.max_bins,\n",
    "                subsample=self.subsample\n",
    "            )\n",
    "            \n",
    "            xgb_binary.fit(X, y_binary)\n",
    "            self.classifiers.append(xgb_binary)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Get probabilities from all classifiers\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes))\n",
    "        \n",
    "        for class_idx, classifier in enumerate(self.classifiers):\n",
    "            # Get raw scores and convert to probabilities\n",
    "            raw_scores = np.zeros(X.shape[0])\n",
    "            for tree in classifier.trees:\n",
    "                raw_scores += classifier.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Sigmoid to get probabilities\n",
    "            proba = 1.0 / (1.0 + np.exp(-raw_scores))\n",
    "            probabilities[:, class_idx] = proba\n",
    "            \n",
    "        # Normalize to make sure probabilities sum to 1\n",
    "        probabilities = probabilities / np.sum(probabilities, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "# Test XGBoost with subsampling for speed\n",
    "print(\"=\"*60)\n",
    "print(\"XGBOOST MODEL TESTING WITH SUBSAMPLING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use smaller settings with subsampling for reasonable training time\n",
    "xgb = MultiClassXGBoost(\n",
    "    n_estimators=20,        # Small number of trees\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    reg_lambda=1.0,\n",
    "    subsample=0.1,          # Use only 60% of data for each tree - MAJOR SPEEDUP!\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting XGBoost training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "xgb.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_xgb = xgb.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")\n",
    "\n",
    "# Calculate metrics\n",
    "f1_weighted_xgb = f1_score(y_val, y_pred_xgb, average='weighted')\n",
    "accuracy_xgb = np.mean(y_pred_xgb == y_val)\n",
    "\n",
    "print(f\"\\nðŸ“Š XGBoost Performance:\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_xgb:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"Total Time: {training_time + pred_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5843bc7e",
   "metadata": {},
   "source": [
    "subsample = 0.6\n",
    "n_est = 30, depth = 3 --- 10min 24sec --- 0.876\n",
    "subsample = 0.1\n",
    "n_est = 30, depth = 3 --- 1min 6sec --- 0.876\n",
    "n_est = 50, depth = 3 --- 6min 22 sec --- 0.892\n",
    "n_est = 30, depth = 4 --- 5min 10 sec --- 0.8896\n",
    "n_est = 20, depth = 4 --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "56e28279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "XGBOOST PERFORMANCE METRICS\n",
      "============================================================\n",
      "F1 Score (Macro): 0.8835\n",
      "F1 Score (Weighted): 0.8843\n",
      "Accuracy: 0.8844\n",
      "Training Time: 644.71s\n",
      "Prediction Time: 2.9117s\n",
      "\n",
      "ðŸ†š COMPARISON WITH ALL MODELS:\n",
      "KNN F1:          0.9523\n",
      "SVM F1:          0.8934\n",
      "Softmax F1:      0.8899\n",
      "Random Forest F1: 0.8503\n",
      "Naive Bayes F1:   0.5424\n",
      "XGBoost F1:      0.8843\n",
      "\n",
      "â° REMAINING TIME BUDGET: -726.03s (-12.10min)\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBOOST PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "f1_macro_xgb = f1_score(y_val, y_pred_xgb, average='macro')\n",
    "f1_weighted_xgb = f1_score(y_val, y_pred_xgb, average='weighted')\n",
    "accuracy_xgb = np.mean(y_pred_xgb == y_val)\n",
    "\n",
    "print(f\"F1 Score (Macro): {f1_macro_xgb:.4f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_xgb:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"Training Time: {training_time:.2f}s\")\n",
    "print(f\"Prediction Time: {pred_time:.4f}s\")\n",
    "\n",
    "# Compare with all models\n",
    "print(f\"\\nðŸ†š COMPARISON WITH ALL MODELS:\")\n",
    "print(f\"KNN F1:          0.9523\")\n",
    "print(f\"SVM F1:          0.8934\")\n",
    "print(f\"Softmax F1:      0.8899\")\n",
    "print(f\"Random Forest F1: 0.8503\")\n",
    "print(f\"Naive Bayes F1:   0.5424\")\n",
    "print(f\"XGBoost F1:      {f1_weighted_xgb:.4f}\")\n",
    "\n",
    "# Time analysis\n",
    "total_xgb_time = training_time + pred_time\n",
    "total_used = 8.11 + 163.38 + 134.94 + 71.98 + total_xgb_time\n",
    "remaining_time = 300 - total_used\n",
    "print(f\"\\nâ° REMAINING TIME BUDGET: {remaining_time:.2f}s ({remaining_time/60:.2f}min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bf476a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTIMIZED XGBOOST TESTING\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Starting OPTIMIZED XGBoost training...\n",
      "Training XGBoost for class 1/10...\n",
      "Training XGBoost for class 2/10...\n",
      "Training XGBoost for class 3/10...\n",
      "Training XGBoost for class 4/10...\n",
      "Training XGBoost for class 5/10...\n",
      "Training XGBoost for class 6/10...\n",
      "Training XGBoost for class 7/10...\n",
      "Training XGBoost for class 8/10...\n",
      "Training XGBoost for class 9/10...\n",
      "Training XGBoost for class 10/10...\n",
      "âœ… Training completed in 140.06 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions...\n",
      "âœ… Prediction completed in 7.5886 seconds\n",
      "\n",
      "ðŸ“Š OPTIMIZED XGBoost Performance:\n",
      "F1 Score (Weighted): 0.9351\n",
      "Accuracy: 0.9352\n",
      "Total Time: 147.65s\n",
      "Expected F1: 0.92-0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class FastXGBoostTree:\n",
    "    def __init__(self, max_depth=3, reg_lambda=1.0, gamma=0.0, max_bins=50):\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.max_bins = max_bins\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.leaf_value = None\n",
    "\n",
    "    def _calc_leaf_value(self, G, H):\n",
    "        return -G / (H + self.reg_lambda)\n",
    "\n",
    "    def _calc_gain(self, G_left, H_left, G_right, H_right, G_total, H_total):\n",
    "        gain = 0.5 * (G_left*G_left/(H_left + self.reg_lambda) + \n",
    "                     G_right*G_right/(H_right + self.reg_lambda) - \n",
    "                     G_total*G_total/(H_total + self.reg_lambda)) - self.gamma\n",
    "        return gain\n",
    "\n",
    "    def _find_best_split(self, X, g, h, sample_indices, feature_thresholds):\n",
    "        G_total = np.sum(g[sample_indices])\n",
    "        H_total = np.sum(h[sample_indices])\n",
    "        \n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        # Only check random subset of features (like Random Forest)\n",
    "        n_features = len(feature_thresholds)\n",
    "        n_features_to_check = int(np.sqrt(n_features))  # Random Forest style\n",
    "        \n",
    "        feature_indices = np.random.choice(n_features, n_features_to_check, replace=False)\n",
    "        \n",
    "        for feat_idx in feature_indices:\n",
    "            thresholds = feature_thresholds[feat_idx]\n",
    "            if thresholds is None or len(thresholds) == 0:\n",
    "                continue\n",
    "\n",
    "            x_col = X[sample_indices, feat_idx]\n",
    "            g_vals = g[sample_indices]\n",
    "            h_vals = h[sample_indices]\n",
    "\n",
    "            # Sort only once per feature\n",
    "            sorted_idx = np.argsort(x_col)\n",
    "            x_sorted = x_col[sorted_idx]\n",
    "            g_sorted = g_vals[sorted_idx]\n",
    "            h_sorted = h_vals[sorted_idx]\n",
    "\n",
    "            G_left, H_left = 0.0, 0.0\n",
    "            \n",
    "            # Try fewer thresholds for speed\n",
    "            step = max(1, len(thresholds) // 10)  # Check only 10 thresholds per feature\n",
    "            for th in thresholds[::step]:\n",
    "                # Find split position efficiently\n",
    "                split_pos = np.searchsorted(x_sorted, th, side='right')\n",
    "                if split_pos == 0 or split_pos == len(x_sorted):\n",
    "                    continue\n",
    "\n",
    "                # Update cumulative sums incrementally\n",
    "                if split_pos > len(g_sorted) // 2:\n",
    "                    # Calculate from the right side for better numerical stability\n",
    "                    G_right = np.sum(g_sorted[split_pos:])\n",
    "                    H_right = np.sum(h_sorted[split_pos:])\n",
    "                    G_left = G_total - G_right\n",
    "                    H_left = H_total - H_right\n",
    "                else:\n",
    "                    G_left = np.sum(g_sorted[:split_pos])\n",
    "                    H_left = np.sum(h_sorted[:split_pos])\n",
    "                    G_right = G_total - G_left\n",
    "                    H_right = H_total - H_left\n",
    "\n",
    "                gain = self._calc_gain(G_left, H_left, G_right, H_right, G_total, H_total)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feat_idx\n",
    "                    best_threshold = th\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def _fit_recursive(self, X, g, h, sample_indices, depth, feature_thresholds):\n",
    "        n_samples = len(sample_indices)\n",
    "        if n_samples == 0:\n",
    "            return\n",
    "\n",
    "        G = np.sum(g[sample_indices])\n",
    "        H = np.sum(h[sample_indices])\n",
    "        self.leaf_value = self._calc_leaf_value(G, H)\n",
    "\n",
    "        # Stop early if no improvement\n",
    "        if depth >= self.max_depth or n_samples < 20:\n",
    "            return\n",
    "\n",
    "        feature, threshold, gain = self._find_best_split(X, g, h, sample_indices, feature_thresholds)\n",
    "\n",
    "        if feature is None or gain <= 0:\n",
    "            return\n",
    "\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "\n",
    "        left_mask = X[sample_indices, feature] <= threshold\n",
    "        left_indices = sample_indices[left_mask]\n",
    "        right_indices = sample_indices[~left_mask]\n",
    "\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "            return\n",
    "\n",
    "        # Grow left and right branches\n",
    "        self.left = FastXGBoostTree(\n",
    "            max_depth=self.max_depth,\n",
    "            reg_lambda=self.reg_lambda,\n",
    "            gamma=self.gamma,\n",
    "            max_bins=self.max_bins\n",
    "        )\n",
    "        self.left._fit_recursive(X, g, h, left_indices, depth + 1, feature_thresholds)\n",
    "\n",
    "        self.right = FastXGBoostTree(\n",
    "            max_depth=self.max_depth,\n",
    "            reg_lambda=self.reg_lambda,\n",
    "            gamma=self.gamma,\n",
    "            max_bins=self.max_bins\n",
    "        )\n",
    "        self.right._fit_recursive(X, g, h, right_indices, depth + 1, feature_thresholds)\n",
    "\n",
    "    def fit(self, X, g, h, feature_thresholds):\n",
    "        sample_indices = np.arange(X.shape[0])\n",
    "        self._fit_recursive(X, g, h, sample_indices, 0, feature_thresholds)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.feature is None:\n",
    "            return np.full(X.shape[0], self.leaf_value)\n",
    "        \n",
    "        left_mask = X[:, self.feature] <= self.threshold\n",
    "        preds = np.full(X.shape[0], self.leaf_value)\n",
    "        \n",
    "        if self.left:\n",
    "            preds[left_mask] = self.left.predict(X[left_mask])\n",
    "        if self.right:\n",
    "            preds[~left_mask] = self.right.predict(X[~left_mask])\n",
    "            \n",
    "        return preds\n",
    "\n",
    "\n",
    "class FastXGBoostClassifier:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3, reg_lambda=1.0, gamma=0.0, max_bins=50, subsample=0.8, colsample=0.8):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.max_bins = max_bins\n",
    "        self.subsample = subsample\n",
    "        self.colsample = colsample  # Column subsampling\n",
    "        self.trees = []\n",
    "        self.base_score = 0.0\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        # Faster sigmoid approximation\n",
    "        x = np.clip(x, -20, 20)  # Tighter bounds for stability\n",
    "        return 0.5 * (x / (1 + np.abs(x))) + 0.5\n",
    "\n",
    "    def _precompute_feature_thresholds(self, X):\n",
    "        thresholds = []\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for feat in range(X.shape[1]):\n",
    "            col = X[:, feat]\n",
    "            if np.all(col == col[0]):\n",
    "                thresholds.append(None)\n",
    "                continue\n",
    "                \n",
    "            # Use fewer bins for speed\n",
    "            n_bins = min(self.max_bins, n_samples // 10)\n",
    "            if len(np.unique(col)) > n_bins:\n",
    "                percentiles = np.linspace(10, 90, n_bins - 1)  # Skip extremes\n",
    "                ths = np.percentile(col, percentiles)\n",
    "            else:\n",
    "                unique_vals = np.unique(col)\n",
    "                ths = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n",
    "                \n",
    "            thresholds.append(ths if len(ths) > 0 else None)\n",
    "        return thresholds\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=np.float32)  # Use float32 for speed\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "        y_binary = y\n",
    "\n",
    "        # Precompute global thresholds once\n",
    "        feature_thresholds = self._precompute_feature_thresholds(X)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        y_pred = np.full(n_samples, self.base_score, dtype=np.float32)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Row subsampling\n",
    "            if self.subsample < 1.0:\n",
    "                subsample_size = int(self.subsample * n_samples)\n",
    "                row_indices = np.random.choice(n_samples, subsample_size, replace=False)\n",
    "            else:\n",
    "                row_indices = np.arange(n_samples)\n",
    "\n",
    "            # Column subsampling for extra speed and regularization\n",
    "            if self.colsample < 1.0:\n",
    "                n_cols = int(self.colsample * n_features)\n",
    "                col_indices = np.random.choice(n_features, n_cols, replace=False)\n",
    "                X_sub = X[row_indices][:, col_indices]\n",
    "                feature_thresholds_sub = [feature_thresholds[i] for i in col_indices]\n",
    "            else:\n",
    "                col_indices = np.arange(n_features)\n",
    "                X_sub = X[row_indices]\n",
    "                feature_thresholds_sub = feature_thresholds\n",
    "\n",
    "            y_sub = y_binary[row_indices]\n",
    "            y_pred_sub = y_pred[row_indices]\n",
    "\n",
    "            # Calculate gradients and hessians\n",
    "            p = self._sigmoid(y_pred_sub)\n",
    "            g = p - y_sub\n",
    "            h = p * (1.0 - p) + 1e-6  # Smaller epsilon\n",
    "\n",
    "            # Train tree\n",
    "            tree = FastXGBoostTree(\n",
    "                max_depth=self.max_depth,\n",
    "                reg_lambda=self.reg_lambda,\n",
    "                gamma=self.gamma,\n",
    "                max_bins=self.max_bins\n",
    "            )\n",
    "            tree.fit(X_sub, g, h, feature_thresholds_sub)\n",
    "            \n",
    "            # Store column indices for prediction\n",
    "            tree.col_indices = col_indices\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Update predictions\n",
    "            if self.colsample < 1.0:\n",
    "                X_pred = X[:, col_indices]\n",
    "            else:\n",
    "                X_pred = X\n",
    "                \n",
    "            update = tree.predict(X_pred)\n",
    "            y_pred += self.learning_rate * update\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        y_pred = np.full(X.shape[0], self.base_score, dtype=np.float32)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            if hasattr(tree, 'col_indices'):\n",
    "                X_sub = X[:, tree.col_indices]\n",
    "            else:\n",
    "                X_sub = X\n",
    "            y_pred += self.learning_rate * tree.predict(X_sub)\n",
    "            \n",
    "        proba = self._sigmoid(y_pred)\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "class FastMultiClassXGBoost:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3, reg_lambda=1.0, \n",
    "                 gamma=0.0, max_bins=50, n_classes=10, subsample=0.8, colsample=0.8):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.max_bins = max_bins\n",
    "        self.n_classes = n_classes\n",
    "        self.subsample = subsample\n",
    "        self.colsample = colsample\n",
    "        self.classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classifiers = []\n",
    "        \n",
    "        for class_idx in range(self.n_classes):\n",
    "            print(f\"Training XGBoost for class {class_idx+1}/{self.n_classes}...\")\n",
    "            \n",
    "            y_binary = (y == class_idx).astype(int)\n",
    "            \n",
    "            xgb_binary = FastXGBoostClassifier(\n",
    "                n_estimators=self.n_estimators,\n",
    "                learning_rate=self.learning_rate,\n",
    "                max_depth=self.max_depth,\n",
    "                reg_lambda=self.reg_lambda,\n",
    "                gamma=self.gamma,\n",
    "                max_bins=self.max_bins,\n",
    "                subsample=self.subsample,\n",
    "                colsample=self.colsample\n",
    "            )\n",
    "            \n",
    "            xgb_binary.fit(X, y_binary)\n",
    "            self.classifiers.append(xgb_binary)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes), dtype=np.float32)\n",
    "        \n",
    "        for class_idx, classifier in enumerate(self.classifiers):\n",
    "            raw_scores = np.zeros(X.shape[0], dtype=np.float32)\n",
    "            for tree in classifier.trees:\n",
    "                if hasattr(tree, 'col_indices'):\n",
    "                    X_sub = X[:, tree.col_indices]\n",
    "                else:\n",
    "                    X_sub = X\n",
    "                raw_scores += classifier.learning_rate * tree.predict(X_sub)\n",
    "            \n",
    "            # Use fast sigmoid\n",
    "            proba = 0.5 * (raw_scores / (1 + np.abs(raw_scores))) + 0.5\n",
    "            probabilities[:, class_idx] = proba\n",
    "            \n",
    "        # Normalize\n",
    "        prob_sums = np.sum(probabilities, axis=1, keepdims=True)\n",
    "        probabilities /= np.maximum(prob_sums, 1e-8)\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "# Test OPTIMIZED XGBoost\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMIZED XGBOOST TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Optimized settings for speed AND accuracy\n",
    "xgb_fast = FastMultiClassXGBoost(\n",
    "    n_estimators=60,           # Balanced number\n",
    "    learning_rate=0.15,        # Slightly higher for faster convergence\n",
    "    max_depth=7,               # Optimal depth\n",
    "    reg_lambda=0.8,            # Slight regularization\n",
    "    subsample=0.7,             # Good balance\n",
    "    colsample=0.4,             # Column subsampling for extra speed\n",
    "    max_bins=32,               # Faster with good accuracy\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting OPTIMIZED XGBoost training...\")\n",
    "train_start = time.time()\n",
    "xgb_fast.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions...\")\n",
    "pred_start = time.time()\n",
    "y_pred_xgb = xgb_fast.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")\n",
    "\n",
    "# Calculate metrics\n",
    "f1_weighted_xgb = f1_score(y_val, y_pred_xgb, average='weighted')\n",
    "accuracy_xgb = np.mean(y_pred_xgb == y_val)\n",
    "\n",
    "print(f\"\\nðŸ“Š OPTIMIZED XGBoost Performance:\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_xgb:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"Total Time: {training_time + pred_time:.2f}s\")\n",
    "print(f\"Expected F1: 0.92-0.95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72940b",
   "metadata": {},
   "source": [
    "0.8\n",
    "30,3,78.86,0.85\n",
    "30,4,90.82,0.87\n",
    "20,4,85.56,0.86\n",
    "30,5,100.73,0.9082\n",
    "40,5,203.39,0.9163\n",
    "50,5,154.95,0.9198\n",
    "50,6,250.25,0.9294\n",
    "0.6\n",
    "50,6,108.32,0.9246\n",
    "0.4\n",
    "60,6,107.46,0.9316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "117f10a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SVM MODEL TESTING - ONE vs REST\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Starting SVM training (this may take a while)...\n",
      "Training SVM for class 0...\n",
      "Training SVM for class 1...\n",
      "Training SVM for class 2...\n",
      "Training SVM for class 3...\n",
      "Training SVM for class 4...\n",
      "Training SVM for class 5...\n",
      "Training SVM for class 6...\n",
      "Training SVM for class 7...\n",
      "Training SVM for class 8...\n",
      "Training SVM for class 9...\n",
      "âœ… Training completed in 49.50 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.0161 seconds\n"
     ]
    }
   ],
   "source": [
    "class MultiClassSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000, n_classes=10):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.n_classes = n_classes\n",
    "        self.W = None  # Weight matrix for all classes\n",
    "        self.b = None  # Bias vector for all classes\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.W = np.zeros((self.n_classes, n_features))\n",
    "        self.b = np.zeros(self.n_classes)\n",
    "\n",
    "        # One-vs-Rest approach\n",
    "        for class_idx in range(self.n_classes):\n",
    "            print(f\"Training SVM for class {class_idx}...\")\n",
    "            # Convert to binary labels: current class vs all others\n",
    "            y_binary = np.where(y == class_idx, 1, -1)\n",
    "            \n",
    "            w = np.zeros(n_features)\n",
    "            b = 0\n",
    "\n",
    "            for _ in range(self.n_iters):\n",
    "                for i, x_i in enumerate(X):\n",
    "                    condition = y_binary[i] * (np.dot(x_i, w) - b) >= 1\n",
    "                    if condition:\n",
    "                        w -= self.lr * (self.lambda_param * w)\n",
    "                    else:\n",
    "                        w -= self.lr * (self.lambda_param * w - np.dot(x_i, y_binary[i]))\n",
    "                        b -= self.lr * y_binary[i]\n",
    "\n",
    "            self.W[class_idx] = w\n",
    "            self.b[class_idx] = b\n",
    "\n",
    "    def predict(self, X):\n",
    "        # For each sample, find the class with highest score\n",
    "        scores = np.dot(X, self.W.T) - self.b\n",
    "        return np.argmax(scores, axis=1)\n",
    "\n",
    "# Test SVM\n",
    "print(\"=\"*60)\n",
    "print(\"SVM MODEL TESTING - ONE vs REST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize SVM with fewer iterations for speed\n",
    "svm = MultiClassSVM(\n",
    "    learning_rate=0.001,\n",
    "    lambda_param=0.01, \n",
    "    n_iters=100,  # Reduced for speed\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting SVM training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c6eb1074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SVM PERFORMANCE METRICS\n",
      "============================================================\n",
      "F1 Score (Macro): 0.8922\n",
      "F1 Score (Weighted): 0.8934\n",
      "Accuracy: 0.8948\n",
      "Training Time: 49.50s\n",
      "Prediction Time: 0.0161s\n",
      "\n",
      "ðŸ†š COMPARISON WITH ALL MODELS:\n",
      "KNN F1:          0.9523\n",
      "Softmax F1:      0.8899\n",
      "Random Forest F1: 0.8503\n",
      "Naive Bayes F1:   0.5424\n",
      "SVM F1:          0.8934\n",
      "\n",
      "â° REMAINING TIME BUDGET: -55.95s (-0.93min)\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVM PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "f1_macro_svm = f1_score(y_val, y_pred_svm, average='macro')\n",
    "f1_weighted_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "accuracy_svm = np.mean(y_pred_svm == y_val)\n",
    "\n",
    "print(f\"F1 Score (Macro): {f1_macro_svm:.4f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_svm:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_svm:.4f}\")\n",
    "print(f\"Training Time: {training_time:.2f}s\")\n",
    "print(f\"Prediction Time: {pred_time:.4f}s\")\n",
    "\n",
    "# Compare with all models\n",
    "print(f\"\\nðŸ†š COMPARISON WITH ALL MODELS:\")\n",
    "print(f\"KNN F1:          0.9523\")\n",
    "print(f\"Softmax F1:      0.8899\")\n",
    "print(f\"Random Forest F1: 0.8503\")\n",
    "print(f\"Naive Bayes F1:   0.5424\")\n",
    "print(f\"SVM F1:          {f1_weighted_svm:.4f}\")\n",
    "\n",
    "# Time analysis\n",
    "total_svm_time = training_time + pred_time\n",
    "total_used = 8.11 + 163.38 + 134.94 + total_svm_time\n",
    "remaining_time = 300 - total_used\n",
    "print(f\"\\nâ° REMAINING TIME BUDGET: {remaining_time:.2f}s ({remaining_time/60:.2f}min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83f5941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting SVM training (this may take a while)...\n",
      "Training SVM for class 0...\n",
      "Training SVM for class 1...\n",
      "Training SVM for class 2...\n",
      "Training SVM for class 3...\n",
      "Training SVM for class 4...\n",
      "Training SVM for class 5...\n",
      "Training SVM for class 6...\n",
      "Training SVM for class 7...\n",
      "Training SVM for class 8...\n",
      "Training SVM for class 9...\n",
      "âœ… Training completed in 15.45 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.0022 seconds\n",
      "F1 Score (Weighted): 0.8575\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter combinations to try (ordered by likely performance)\n",
    "param_combinations = [\n",
    "    # Fastest first, then more accurate but slower\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.1, 'n_iters': 30},\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.01, 'n_iters': 40},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.05, 'n_iters': 50},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.01, 'n_iters': 60},\n",
    "    {'learning_rate': 0.001, 'lambda_param': 0.01, 'n_iters': 80},\n",
    "]\n",
    "svm = MultiClassSVM(\n",
    "    learning_rate=0.01,\n",
    "    lambda_param=0.01, \n",
    "    n_iters=30,  # Reduced for speed\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting SVM training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")\n",
    "f1_weighted_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "359c6c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting SVM training (this may take a while)...\n",
      "Training SVM for class 0...\n",
      "Training SVM for class 1...\n",
      "Training SVM for class 2...\n",
      "Training SVM for class 3...\n",
      "Training SVM for class 4...\n",
      "Training SVM for class 5...\n",
      "Training SVM for class 6...\n",
      "Training SVM for class 7...\n",
      "Training SVM for class 8...\n",
      "Training SVM for class 9...\n",
      "âœ… Training completed in 21.45 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.0035 seconds\n",
      "F1 Score (Weighted): 0.8605\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter combinations to try (ordered by likely performance)\n",
    "param_combinations = [\n",
    "    # Fastest first, then more accurate but slower\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.1, 'n_iters': 30},\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.01, 'n_iters': 40},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.05, 'n_iters': 50},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.01, 'n_iters': 60},\n",
    "    {'learning_rate': 0.001, 'lambda_param': 0.01, 'n_iters': 80},\n",
    "]\n",
    "svm = MultiClassSVM(\n",
    "    learning_rate=0.01,\n",
    "    lambda_param=0.01, \n",
    "    n_iters=40,  # Reduced for speed\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting SVM training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")\n",
    "f1_weighted_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7bab5bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting SVM training (this may take a while)...\n",
      "Training SVM for class 0...\n",
      "Training SVM for class 1...\n",
      "Training SVM for class 2...\n",
      "Training SVM for class 3...\n",
      "Training SVM for class 4...\n",
      "Training SVM for class 5...\n",
      "Training SVM for class 6...\n",
      "Training SVM for class 7...\n",
      "Training SVM for class 8...\n",
      "Training SVM for class 9...\n",
      "âœ… Training completed in 26.44 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.0020 seconds\n",
      "F1 Score (Weighted): 0.8271\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter combinations to try (ordered by likely performance)\n",
    "param_combinations = [\n",
    "    # Fastest first, then more accurate but slower\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.1, 'n_iters': 30},\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.01, 'n_iters': 40},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.05, 'n_iters': 50},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.01, 'n_iters': 60},\n",
    "    {'learning_rate': 0.001, 'lambda_param': 0.01, 'n_iters': 80},\n",
    "]\n",
    "svm = MultiClassSVM(\n",
    "    learning_rate=0.005,\n",
    "    lambda_param=0.05, \n",
    "    n_iters=50,  # Reduced for speed\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting SVM training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")\n",
    "f1_weighted_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f7e0a21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting SVM training (this may take a while)...\n",
      "Training SVM for class 0...\n",
      "Training SVM for class 1...\n",
      "Training SVM for class 2...\n",
      "Training SVM for class 3...\n",
      "Training SVM for class 4...\n",
      "Training SVM for class 5...\n",
      "Training SVM for class 6...\n",
      "Training SVM for class 7...\n",
      "Training SVM for class 8...\n",
      "Training SVM for class 9...\n",
      "âœ… Training completed in 38.12 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.0000 seconds\n",
      "F1 Score (Weighted): 0.8752\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter combinations to try (ordered by likely performance)\n",
    "param_combinations = [\n",
    "    # Fastest first, then more accurate but slower\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.1, 'n_iters': 30},\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.01, 'n_iters': 40},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.05, 'n_iters': 50},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.01, 'n_iters': 60},\n",
    "    {'learning_rate': 0.001, 'lambda_param': 0.01, 'n_iters': 80},\n",
    "]\n",
    "svm = MultiClassSVM(\n",
    "    learning_rate=0.005,\n",
    "    lambda_param=0.01, \n",
    "    n_iters=60,  # Reduced for speed\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting SVM training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")\n",
    "f1_weighted_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9ea7dd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting SVM training (this may take a while)...\n",
      "Training SVM for class 0...\n",
      "Training SVM for class 1...\n",
      "Training SVM for class 2...\n",
      "Training SVM for class 3...\n",
      "Training SVM for class 4...\n",
      "Training SVM for class 5...\n",
      "Training SVM for class 6...\n",
      "Training SVM for class 7...\n",
      "Training SVM for class 8...\n",
      "Training SVM for class 9...\n",
      "âœ… Training completed in 50.37 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.0000 seconds\n",
      "F1 Score (Weighted): 0.8930\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter combinations to try (ordered by likely performance)\n",
    "param_combinations = [\n",
    "    # Fastest first, then more accurate but slower\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.1, 'n_iters': 30},\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.01, 'n_iters': 40},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.05, 'n_iters': 50},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.01, 'n_iters': 60},\n",
    "    {'learning_rate': 0.001, 'lambda_param': 0.01, 'n_iters': 80},\n",
    "]\n",
    "svm = MultiClassSVM(\n",
    "    learning_rate=0.001,\n",
    "    lambda_param=0.01, \n",
    "    n_iters=80,  # Reduced for speed\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting SVM training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")\n",
    "f1_weighted_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0cc2b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting SVM training (this may take a while)...\n",
      "Training SVM for class 0...\n",
      "Training SVM for class 1...\n",
      "Training SVM for class 2...\n",
      "Training SVM for class 3...\n",
      "Training SVM for class 4...\n",
      "Training SVM for class 5...\n",
      "Training SVM for class 6...\n",
      "Training SVM for class 7...\n",
      "Training SVM for class 8...\n",
      "Training SVM for class 9...\n",
      "âœ… Training completed in 57.38 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.0000 seconds\n",
      "F1 Score (Weighted): 0.8957\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter combinations to try (ordered by likely performance)\n",
    "param_combinations = [\n",
    "    # Fastest first, then more accurate but slower\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.1, 'n_iters': 30},\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.01, 'n_iters': 40},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.05, 'n_iters': 50},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.01, 'n_iters': 60},\n",
    "    {'learning_rate': 0.001, 'lambda_param': 0.01, 'n_iters': 80},\n",
    "]\n",
    "svm = MultiClassSVM(\n",
    "    learning_rate=0.0005,\n",
    "    lambda_param=0.01, \n",
    "    n_iters=100,  # Reduced for speed\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting SVM training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")\n",
    "f1_weighted_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "84851d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting SVM training (this may take a while)...\n",
      "Training SVM for class 0...\n",
      "Training SVM for class 1...\n",
      "Training SVM for class 2...\n",
      "Training SVM for class 3...\n",
      "Training SVM for class 4...\n",
      "Training SVM for class 5...\n",
      "Training SVM for class 6...\n",
      "Training SVM for class 7...\n",
      "Training SVM for class 8...\n",
      "Training SVM for class 9...\n",
      "âœ… Training completed in 65.63 seconds\n",
      "\n",
      "ðŸŽ¯ Making predictions on validation set...\n",
      "âœ… Prediction completed in 0.0068 seconds\n",
      "F1 Score (Weighted): 0.9023\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter combinations to try (ordered by likely performance)\n",
    "param_combinations = [\n",
    "    # Fastest first, then more accurate but slower\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.1, 'n_iters': 30},\n",
    "    {'learning_rate': 0.01, 'lambda_param': 0.01, 'n_iters': 40},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.05, 'n_iters': 50},\n",
    "    {'learning_rate': 0.005, 'lambda_param': 0.01, 'n_iters': 60},\n",
    "    {'learning_rate': 0.001, 'lambda_param': 0.01, 'n_iters': 80},\n",
    "]\n",
    "svm = MultiClassSVM(\n",
    "    learning_rate=0.0003,\n",
    "    lambda_param=0.008, \n",
    "    n_iters=120,  # Reduced for speed\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nðŸš€ Starting SVM training (this may take a while)...\")\n",
    "train_start = time.time()\n",
    "svm.fit(X_train, y_train)\n",
    "training_time = time.time() - train_start\n",
    "print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nðŸŽ¯ Making predictions on validation set...\")\n",
    "pred_start = time.time()\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "print(f\"âœ… Prediction completed in {pred_time:.4f} seconds\")\n",
    "f1_weighted_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "24c5dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PCA-WeightedKNN with optimal dimensions...\n",
      "============================================================\n",
      "FINDING OPTIMAL PCA DIMENSIONS FOR WEIGHTED KNN\n",
      "============================================================\n",
      "\n",
      "Testing PCA with 30 components...\n",
      "Applying PCA to reduce dimensions from 784 to 30...\n",
      "PCA completed in 0.32s\n",
      "  F1 Score: 0.9583\n",
      "  Total Time: 0.84s\n",
      "  Dimensions: 784 â†’ 30\n",
      "  ðŸŽ¯ NEW BEST!\n",
      "\n",
      "Testing PCA with 50 components...\n",
      "Applying PCA to reduce dimensions from 784 to 50...\n",
      "PCA completed in 0.32s\n",
      "  F1 Score: 0.9579\n",
      "  Total Time: 0.95s\n",
      "  Dimensions: 784 â†’ 50\n",
      "\n",
      "Testing PCA with 80 components...\n",
      "Applying PCA to reduce dimensions from 784 to 80...\n",
      "PCA completed in 0.31s\n",
      "  F1 Score: 0.9571\n",
      "  Total Time: 0.84s\n",
      "  Dimensions: 784 â†’ 80\n",
      "\n",
      "Testing PCA with 100 components...\n",
      "Applying PCA to reduce dimensions from 784 to 100...\n",
      "PCA completed in 0.29s\n",
      "  F1 Score: 0.9539\n",
      "  Total Time: 0.82s\n",
      "  Dimensions: 784 â†’ 100\n",
      "\n",
      "Testing PCA with 150 components...\n",
      "Applying PCA to reduce dimensions from 784 to 150...\n",
      "PCA completed in 0.29s\n",
      "  F1 Score: 0.9535\n",
      "  Total Time: 0.89s\n",
      "  Dimensions: 784 â†’ 150\n",
      "\n",
      "==================================================\n",
      "OPTIMAL PCA DIMENSIONS: 30\n",
      "BEST F1 SCORE: 0.9583\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "FINAL PCA-WEIGHTED KNN PERFORMANCE\n",
      "============================================================\n",
      "Applying PCA to reduce dimensions from 784 to 30...\n",
      "PCA completed in 0.35s\n",
      "F1 Score: 0.9583\n",
      "Accuracy: 0.9584\n",
      "Training Time (incl PCA): 0.35s\n",
      "Prediction Time: 0.74s\n",
      "Total Time: 1.09s\n",
      "PCA Dimensions: 784 â†’ 30\n",
      "Speedup vs original: ~150.1x faster!\n",
      "\n",
      "Comparison with original KNN:\n",
      "Original KNN F1: 0.9523\n",
      "PCA KNN F1:      0.9583\n",
      "Difference:      +0.0060\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Center the data\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "        \n",
    "        # Compute covariance matrix\n",
    "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "        \n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "        \n",
    "        # Sort eigenvectors by eigenvalues in descending order\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        self.components = eigenvectors[:, sorted_indices[:self.n_components]]\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        return np.dot(X_centered, self.components)\n",
    "\n",
    "class PCAWeightedKNN:\n",
    "    def __init__(self, k=5, n_components=50, batch_size=100):\n",
    "        self.k = k\n",
    "        self.n_components = n_components\n",
    "        self.batch_size = batch_size\n",
    "        self.pca = PCA(n_components)\n",
    "        self.X_train_pca = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        print(f\"Applying PCA to reduce dimensions from {X.shape[1]} to {self.n_components}...\")\n",
    "        start_pca = time.time()\n",
    "        self.pca.fit(X)\n",
    "        self.X_train_pca = self.pca.transform(X.astype(np.float32))\n",
    "        self.y_train = y.astype(np.int32)\n",
    "        pca_time = time.time() - start_pca\n",
    "        print(f\"PCA completed in {pca_time:.2f}s\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_pca = self.pca.transform(X.astype(np.float32))\n",
    "        predictions = np.empty(X_pca.shape[0], dtype=np.int32)\n",
    "        \n",
    "        # Precompute squared norms for training data in PCA space\n",
    "        train_norms = np.sum(self.X_train_pca ** 2, axis=1)\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, X_pca.shape[0], self.batch_size):\n",
    "            batch_end = min(batch_start + self.batch_size, X_pca.shape[0])\n",
    "            X_batch = X_pca[batch_start:batch_end]\n",
    "            \n",
    "            # Vectorized distance computation in PCA space\n",
    "            test_norms = np.sum(X_batch ** 2, axis=1)\n",
    "            dot_products = np.dot(self.X_train_pca, X_batch.T)\n",
    "            squared_distances = train_norms[:, np.newaxis] + test_norms - 2 * dot_products\n",
    "            \n",
    "            # Get k nearest neighbors for each batch sample\n",
    "            nearest_indices = np.argpartition(squared_distances, self.k, axis=0)[:self.k]\n",
    "            nearest_squared_distances = np.take_along_axis(squared_distances, nearest_indices, axis=0)\n",
    "            nearest_labels = self.y_train[nearest_indices]\n",
    "            \n",
    "            # Convert to actual distances and compute weights\n",
    "            distances = np.sqrt(nearest_squared_distances + 1e-8)\n",
    "            weights = 1.0 / (distances + 1e-8)\n",
    "            \n",
    "            # Weighted majority voting for each sample in batch\n",
    "            for j in range(batch_end - batch_start):\n",
    "                label_weights = {}\n",
    "                for idx in range(self.k):\n",
    "                    label = nearest_labels[idx, j]\n",
    "                    weight = weights[idx, j]\n",
    "                    label_weights[label] = label_weights.get(label, 0.0) + weight\n",
    "                \n",
    "                predictions[batch_start + j] = max(label_weights.items(), key=lambda x: x[1])[0]\n",
    "                \n",
    "        return predictions\n",
    "\n",
    "# Test different PCA dimensions to find optimal\n",
    "def find_optimal_pca_knn(X_train, y_train, X_val, y_val):\n",
    "    print(\"=\"*60)\n",
    "    print(\"FINDING OPTIMAL PCA DIMENSIONS FOR WEIGHTED KNN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    pca_dims = [30, 50, 80, 100, 150]  # Test different dimensions\n",
    "    best_f1 = 0\n",
    "    best_pca_dim = 0\n",
    "    best_knn = None\n",
    "    \n",
    "    for n_components in pca_dims:\n",
    "        print(f\"\\nTesting PCA with {n_components} components...\")\n",
    "        \n",
    "        knn_pca = PCAWeightedKNN(k=5, n_components=n_components, batch_size=200)\n",
    "        \n",
    "        # Training (includes PCA)\n",
    "        start_time = time.time()\n",
    "        knn_pca.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Prediction\n",
    "        pred_start = time.time()\n",
    "        y_pred = knn_pca.predict(X_val)\n",
    "        pred_time = time.time() - pred_start\n",
    "        \n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        total_time = training_time + pred_time\n",
    "        \n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  Total Time: {total_time:.2f}s\")\n",
    "        print(f\"  Dimensions: {X_train.shape[1]} â†’ {n_components}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_pca_dim = n_components\n",
    "            best_knn = knn_pca\n",
    "            print(f\"  ðŸŽ¯ NEW BEST!\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"OPTIMAL PCA DIMENSIONS: {best_pca_dim}\")\n",
    "    print(f\"BEST F1 SCORE: {best_f1:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return best_knn, best_pca_dim, best_f1\n",
    "\n",
    "# Test with optimal PCA\n",
    "print(\"Testing PCA-WeightedKNN with optimal dimensions...\")\n",
    "optimal_knn, optimal_dim, optimal_f1 = find_optimal_pca_knn(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Final performance with best PCA\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL PCA-WEIGHTED KNN PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# One more run with optimal parameters for accurate timing\n",
    "final_knn = PCAWeightedKNN(k=5, n_components=optimal_dim, batch_size=200)\n",
    "\n",
    "start_time = time.time()\n",
    "final_knn.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "pred_start = time.time()\n",
    "y_pred_final = final_knn.predict(X_val)\n",
    "pred_time = time.time() - pred_start\n",
    "\n",
    "f1_final = f1_score(y_val, y_pred_final, average='weighted')\n",
    "accuracy_final = np.mean(y_pred_final == y_val)\n",
    "\n",
    "print(f\"F1 Score: {f1_final:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_final:.4f}\")\n",
    "print(f\"Training Time (incl PCA): {training_time:.2f}s\")\n",
    "print(f\"Prediction Time: {pred_time:.2f}s\")\n",
    "print(f\"Total Time: {training_time + pred_time:.2f}s\")\n",
    "print(f\"PCA Dimensions: {X_train.shape[1]} â†’ {optimal_dim}\")\n",
    "print(f\"Speedup vs original: ~{163.38/(training_time + pred_time):.1f}x faster!\")\n",
    "\n",
    "# Compare with original KNN\n",
    "print(f\"\\nComparison with original KNN:\")\n",
    "print(f\"Original KNN F1: 0.9523\")\n",
    "print(f\"PCA KNN F1:      {f1_final:.4f}\")\n",
    "print(f\"Difference:      {f1_final - 0.9523:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
